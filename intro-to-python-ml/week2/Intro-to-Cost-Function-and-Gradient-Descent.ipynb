{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an input vector $x^T = (X_1 , X_2 , \\dots , X_p )$, and a real-valued output y, the linear regression model\n",
    "has the form:\n",
    "\n",
    "$$f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j$$\n",
    "\n",
    "We consider a set of training data $(x_1 , y_1 ) \\dots (x_N , y_N )$ from which to estimate the parameters $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/lr.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X (independent variables) and y (dependent variable).\n",
    "\n",
    "The function that defines the difference between your actual value and the predicted value. \n",
    "\n",
    "$$\n",
    "y_t = y_p + e\n",
    "$$\n",
    "\n",
    "In case of Linear regression with:\n",
    "$$y_p = f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$$\n",
    "\n",
    "$$\n",
    "error = y_t - y_p = y_t - (\\beta_0 + \\beta_1X_1 + \\beta_2X_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/error.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Cost funtion (Just for Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squares\n",
    "$$\\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n(y_t - \\beta_0 - \\beta_1X_1 - \\beta_2X_2)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Vanilla) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent is an **optimisation algorithm** that attempts to find the **local or global minima** of a function by using its **partial derivatives**. In ML it is used to __optimise the cost function__ (reduce our error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This algorithm is commonly used to optimise convex functions. The idea is very simple — to reach the global optimum of a convex function from any point, we need to move in the direction **opposite** to that of greatest increase of the function. As the function is convex, this strategy will always take us to the global optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - A gradient is an extension of partial derivatives. Gradients take the partial derivatives of each variable in a function and then places each partial derivative in a vector. The gradient value is zero at a local maximum or local minimum (because there is no single direction of increase) -  also referred to as **convergence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In mathematics, the **gradient is a multi-variable generalization of the derivative**. While a derivative can be defined on functions of a single variable, for functions of several variables, the gradient takes its place. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Gradient** of a function gives the direction of the **steepest ascent**, i.e. the direction to move if you want to increase the function. \n",
    "\n",
    "Now, the _direction_ of **greatest increase of the function** is determined by taking the partial derivatives of the function with respect to every variable. For example, let us say we wish to optimise a convex function $f(x)$, where $x = (x_1, x_2, \\ldots, x_d)^T$. Let us say, we start at a point $s \\in \\mathbb{R}^d$. Then, the direction of greatest increase of $f$ at $s$ is given by \n",
    "\n",
    "$$\n",
    "\\nabla f(s) = \\left(\\frac{\\partial f(s)}{\\partial x_1}, \\frac{\\partial f(s)}{\\partial x_2}, \\ldots ,\\frac{\\partial f(s)}{\\partial x_d} \\right)^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Since we want to **decrease the function**, we take the **negative gradient**. The length of the gradient function is an indication of how step the slope is. \n",
    " \n",
    "With this derivative, we design an update rule, which asks us to move in the direction opposite to the direction of greatest increase. By repeatedly applying this rule, we hope to reach the global minimum. We hence move to $t$, which is given by\n",
    "\n",
    "$$\n",
    "t = s - \\gamma \\nabla f(s),\n",
    "$$\n",
    "where $\\gamma$ is a parameter called the *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/gd1.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create a model with 50 observations and 2 features per observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1=np.random.randint(low=1,high=20,size=(50,))\n",
    "feature_2=np.random.randint(low=1,high=20,size=(50,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1.shape, feature_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=3+2*feature_1-4*feature_2+np.random.random((50,))\n",
    "print(y_true.shape)\n",
    "y_true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame({'intercept':np.ones_like(feature_1),'feature_1':feature_1,'feature_2':feature_2})\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.zeros(X.shape[1])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myprediction(features,weights):\n",
    "    predictions=np.dot(features,weights)\n",
    "    return(predictions)\n",
    "\n",
    "y_pred = myprediction(X,W)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myerror(y_true,y_pred):\n",
    "    error=y_true - y_pred\n",
    "    return(error)\n",
    "\n",
    "myerror(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycost(y_true,y_pred):\n",
    "    error=myerror(y_true,y_pred)\n",
    "    cost=np.dot(error.T,error)\n",
    "    return(cost)\n",
    "\n",
    "mycost(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(y_true,features,weights):\n",
    "    \n",
    "    y_pred = myprediction(features,weights)\n",
    "    error=myerror(y_true,y_pred)    \n",
    "    gradient=-np.dot(features.T,error)  #gradient = -2X^T*(y-XW)\n",
    "    \n",
    "    return(gradient)\n",
    "\n",
    "gradient(y_true,X,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fit(y_true,features,learning_rate):\n",
    "    \n",
    "    weights=np.zeros(features.shape[1])\n",
    "    \n",
    "    for i in np.arange(30000):\n",
    "        \n",
    "        weights = weights - learning_rate*gradient(y_true,features,weights)         \n",
    "#        weights[0] = weights[0] - 10*learning_rate*gradient(y_true,features,weights)[0]\n",
    "        \n",
    "        if i%1000==0:\n",
    "            y_pred = myprediction(features,weights)\n",
    "            print(mycost(y_true,y_pred),weights)\n",
    "            \n",
    "    return(weights)\n",
    "\n",
    "lr_fit(y_true,X,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets implement the same model using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X.iloc[:,1:],y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
