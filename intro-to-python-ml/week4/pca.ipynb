{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Value and Eigen Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every vector (list of numbers) has a direction when it is plotted on a XY chart. \n",
    "- Consider A to be any square matrix. Now, almost all vectors will change direction, when they are multiplied by A. Certain exceptional vectors x are in the same direction as Ax, i.e. the direction of x and Ax will be the same. Those are the “eigenvectors”. Multiply an eigenvector by A, and the vector Ax is a number ($\\lambda$) times the original x. The number ($\\lambda$) is called the \"eigenvalue\".\n",
    "- An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "- Eigen value— The scalar that is used to transform an Eigenvector.\n",
    "- The eigenvalue tells whether the special vector x is stretched or shrunk or reversed or left unchanged—when it is multiplied by A. \n",
    "\n",
    "`A * eigen_vector = eigen_value * eigen_vector` \n",
    "\n",
    "The above equation states that we need to multiply a scalar lambda (eigen_value) to the vector x (eigen_vector) such that it is equal to the linear transformation of matrix A once it is scaled by vector x.\n",
    "\n",
    "if a square matrix has a size n (nxn) then we will get n eigenvalues and as a result, n eigenvectors will be computed to represent the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In an ideal world, if in a given feature vector, all features were statistically independent, one could simply eliminate the least discriminative features from the vector. The least discriminative features can be found by various greedy feature selection approaches. \n",
    "\n",
    "- However, in practice, features are correlated. Many features depend on each other or on an underlying unknown variable. A single feature could therefore represent a combination of multiple types of information by a single value. \n",
    "\n",
    "- Before eliminating features for dimensionality reduction, we would like to transform the complete feature space such that the underlying uncorrelated components are obtained.\n",
    "\n",
    "- The feature space, can be decorrelated by selecting the eigenvectors of the covariance matrix as the new reference axes. We can thus obtain the uncorrelated features. \n",
    "- After this dimensionality reduction can be performed. \n",
    "\n",
    "- PCA simply assumes that the most interesting feature is the one with the largest variance or spread. This assumption is based on an information theoretic point of view, since the dimension with the largest variance corresponds to the dimension with the largest entropy and thus encodes the most information. The smallest eigenvectors will often simply represent noise components, whereas the largest eigenvectors often correspond to the principal components that define the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
